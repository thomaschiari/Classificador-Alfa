{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import plot_tree\n",
    "from ClassificadorAlfa import ClassificadorAlfa\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('data', 'healthcare-dataset-stroke-data.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hipótese 1\n",
    "\n",
    "Para a hipótese 1, vamos testar todos os dados contidos no DataFrame, sem alterações, porém apenas os dados que podem ser tratados como Booleanos. Vamos utilizar as variáveis categóricas para realizar o teste. A hipótese nula, no caso, seria montar um classificador que irá sempre prever os dados como sendo o mais frequente $(x)$, ou seja, será a acurácia medida ao dividir tal valor pelo total dos dados contidos no DataFrame $(N)$:\n",
    "\n",
    "$$\n",
    "Acurácia = \\frac{x}{N}\n",
    "$$\n",
    "\n",
    "Portanto, vamos calcular, ao final, a acurácia da hipótese nula e verificar como a nossa hipótese, utilizando todas as variáveis categóricas, se comporta com relação a ela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Separando target e features\n",
    "try:\n",
    "    df_ = df.drop(['id', 'bmi', 'age', 'avg_glucose_level'], axis=1)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "X = df_.drop('stroke', axis=1)\n",
    "y = df_['stroke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Obtendo variáveis categoricas\n",
    "object_features = [feature for feature in X.columns if X[feature].dtype == 'O']\n",
    "int_features = [feature for feature in X.columns if X[feature].dtype == 'int64']\n",
    "categorical_features = object_features + int_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Obtendo variáveis dummy\n",
    "X = X[categorical_features]\n",
    "\n",
    "X = pd.get_dummies(X)\n",
    "X.hypertension = X.hypertension.astype('bool')\n",
    "X.heart_disease = X.heart_disease.astype('bool')\n",
    "\n",
    "X_names = X.columns\n",
    "\n",
    "y = y.replace(0,-1)\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5)\n",
    "X_train = X_train.astype('float64')\n",
    "y_train = y_train.astype('float64')\n",
    "X_test = X_test.astype('float64')\n",
    "y_test = y_test.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Inicializando os parâmetros do modelo\n",
    "a = np.random.randn(X_train.shape[1], 1)\n",
    "b = 1.0\n",
    "\n",
    "X_train = X_train.T\n",
    "y_train = y_train.T\n",
    "\n",
    "parametros = [a, b, X_train, y_train]\n",
    "learning_rate = 0.0001\n",
    "num_iteracoes = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classificador = ClassificadorAlfa(learning_rate, num_iteracoes, parametros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Treinando o modelo\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m a, b \u001B[38;5;241m=\u001B[39m \u001B[43mclassificador\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtreinar\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m a, b\n",
      "File \u001B[0;32m~/dev/insper/3sem/alglin/aps6/Classificador-Alfa/ClassificadorAlfa.py:67\u001B[0m, in \u001B[0;36mClassificadorAlfa.treinar\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtreinar\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     61\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;124;03m    Treina o modelo para encontrar os melhores valores dos parâmetros.\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;124;03m    \u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;124;03m    Returns:\u001B[39;00m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m    tuple: tupla contendo os valores otimizados dos parâmetros a e b.\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 67\u001B[0m     a, b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmelhorar_modelo\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#otimiza os parâmetros do modelo\u001B[39;00m\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m a, b\n",
      "File \u001B[0;32m~/dev/insper/3sem/alglin/aps6/Classificador-Alfa/ClassificadorAlfa.py:55\u001B[0m, in \u001B[0;36mClassificadorAlfa.melhorar_modelo\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     53\u001B[0m a, b, x, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miters):\n\u001B[0;32m---> 55\u001B[0m     g_ \u001B[38;5;241m=\u001B[39m \u001B[43mg\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#calcula o gradiente da função erro para os parâmetros atuais\u001B[39;00m\n\u001B[1;32m     56\u001B[0m     a \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rate \u001B[38;5;241m*\u001B[39m g_[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;66;03m#atualiza o valor de a usando o gradiente\u001B[39;00m\n\u001B[1;32m     57\u001B[0m     b \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rate \u001B[38;5;241m*\u001B[39m g_[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;66;03m#atualiza o valor de b usando o gradiente\u001B[39;00m\n",
      "File \u001B[0;32m~/dev/insper/3sem/alglin/aps6/Classificador-Alfa/env/lib/python3.10/site-packages/autograd/wrap_util.py:20\u001B[0m, in \u001B[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     19\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(args[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m argnum)\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43munary_operator\u001B[49m\u001B[43m(\u001B[49m\u001B[43munary_f\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mnary_op_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mnary_op_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/dev/insper/3sem/alglin/aps6/Classificador-Alfa/env/lib/python3.10/site-packages/autograd/differential_operators.py:32\u001B[0m, in \u001B[0;36mgrad\u001B[0;34m(fun, x)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m vspace(ans)\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGrad only applies to real scalar-output functions. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     31\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTry jacobian, elementwise_grad or holomorphic_grad.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 32\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mvjp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvspace\u001B[49m\u001B[43m(\u001B[49m\u001B[43mans\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/dev/insper/3sem/alglin/aps6/Classificador-Alfa/env/lib/python3.10/site-packages/autograd/core.py:14\u001B[0m, in \u001B[0;36mmake_vjp.<locals>.vjp\u001B[0;34m(g)\u001B[0m\n\u001B[0;32m---> 14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvjp\u001B[39m(g): \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbackward_pass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_node\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/dev/insper/3sem/alglin/aps6/Classificador-Alfa/env/lib/python3.10/site-packages/autograd/core.py:21\u001B[0m, in \u001B[0;36mbackward_pass\u001B[0;34m(g, end_node)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m toposort(end_node):\n\u001B[1;32m     20\u001B[0m     outgrad \u001B[38;5;241m=\u001B[39m outgrads\u001B[38;5;241m.\u001B[39mpop(node)\n\u001B[0;32m---> 21\u001B[0m     ingrads \u001B[38;5;241m=\u001B[39m \u001B[43mnode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvjp\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutgrad\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m parent, ingrad \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(node\u001B[38;5;241m.\u001B[39mparents, ingrads):\n\u001B[1;32m     23\u001B[0m         outgrads[parent] \u001B[38;5;241m=\u001B[39m add_outgrads(outgrads\u001B[38;5;241m.\u001B[39mget(parent), ingrad)\n",
      "File \u001B[0;32m~/dev/insper/3sem/alglin/aps6/Classificador-Alfa/env/lib/python3.10/site-packages/autograd/core.py:78\u001B[0m, in \u001B[0;36mdefvjp.<locals>.vjp_argnums.<locals>.<lambda>\u001B[0;34m(g)\u001B[0m\n\u001B[1;32m     76\u001B[0m     vjp_0 \u001B[38;5;241m=\u001B[39m vjp_0_fun(ans, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     77\u001B[0m     vjp_1 \u001B[38;5;241m=\u001B[39m vjp_1_fun(ans, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mlambda\u001B[39;00m g: (\u001B[43mvjp_0\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m)\u001B[49m, vjp_1(g))\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     80\u001B[0m     vjps \u001B[38;5;241m=\u001B[39m [vjps_dict[argnum](ans, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;28;01mfor\u001B[39;00m argnum \u001B[38;5;129;01min\u001B[39;00m argnums]\n",
      "File \u001B[0;32m~/dev/insper/3sem/alglin/aps6/Classificador-Alfa/env/lib/python3.10/site-packages/autograd/numpy/numpy_vjps.py:418\u001B[0m, in \u001B[0;36mmatmul_vjp_0.<locals>.<lambda>\u001B[0;34m(g)\u001B[0m\n\u001B[1;32m    416\u001B[0m A_meta \u001B[38;5;241m=\u001B[39m anp\u001B[38;5;241m.\u001B[39mmetadata(A)\n\u001B[1;32m    417\u001B[0m B_ndim \u001B[38;5;241m=\u001B[39m anp\u001B[38;5;241m.\u001B[39mndim(B)\n\u001B[0;32m--> 418\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mlambda\u001B[39;00m g: \u001B[43mmatmul_adjoint_0\u001B[49m\u001B[43m(\u001B[49m\u001B[43mB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mA_meta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mB_ndim\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/dev/insper/3sem/alglin/aps6/Classificador-Alfa/env/lib/python3.10/site-packages/autograd/numpy/numpy_vjps.py:395\u001B[0m, in \u001B[0;36mmatmul_adjoint_0\u001B[0;34m(B, G, A_meta, B_ndim)\u001B[0m\n\u001B[1;32m    393\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# We need to swap the last two axes of B\u001B[39;00m\n\u001B[1;32m    394\u001B[0m     B \u001B[38;5;241m=\u001B[39m anp\u001B[38;5;241m.\u001B[39mswapaxes(B, B_ndim \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m, B_ndim \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 395\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43manp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mG\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mB\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m unbroadcast(result, A_meta)\n",
      "File \u001B[0;32m~/dev/insper/3sem/alglin/aps6/Classificador-Alfa/env/lib/python3.10/site-packages/autograd/tracer.py:48\u001B[0m, in \u001B[0;36mprimitive.<locals>.f_wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m new_box(ans, trace, node)\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf_raw\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Treinando o modelo\n",
    "a, b = classificador.treinar()\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fazendo previsões\n",
    "ypred = a.T @ X_test.T + b\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acuracia = ClassificadorAlfa.acuracia(y_test, ypred)\n",
    "print(f'A acurácia do modelo foi de {acuracia*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Selecionando as features mais importantes\n",
    "features = X_names\n",
    "importances = pd.DataFrame(data=a, index=features, columns=['importance']).sort_values(by='importance', ascending=False)\n",
    "importances"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Obtendo a acurácia da hipótese nula\n",
    "valor_mais_frequente = np.bincount(y).argmax()\n",
    "acuracia_nula = np.mean(y == valor_mais_frequente)\n",
    "\n",
    "print(f'A acurácia da hipótese nula foi de {acuracia_nula*100:.2f}%')\n",
    "print(f'A acurácia do modelo foi de {acuracia*100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utilizando um classificador de árvore de decisão\n",
    "\n",
    "Em seguida, vamos comparar o nosso modelo com um classificador de árvore de decisão, comparando, novamente, as acurácias obtidas."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classificador = DecisionTreeClassifier(criterion='entropy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5)\n",
    "X_train = X_train.astype('float64')\n",
    "y_train = y_train.astype('float64')\n",
    "X_test = X_test.astype('float64')\n",
    "y_test = y_test.astype('float64')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classificador.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classificador.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acuracia = classificador.score(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'A acurácia da hipótese nula foi de {acuracia_nula*100:.2f}%')\n",
    "print(f'A acurácia do modelo foi de {acuracia*100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotando a árvore de decisão\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(classificador, filled=True, rounded=True, class_names=['Não teve AVC', 'Teve AVC'], feature_names=X_names)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}